{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPu9kMbMUQs3hj5S9vbHQIO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1044cc152fe94becaf41fd2efd09f498":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_752deba207af44e59488e7ad1630757e","IPY_MODEL_cb2a89fc42704701a11433890eac0e76","IPY_MODEL_1248fe97e82d413280d8dd153df0fdd7"],"layout":"IPY_MODEL_47db5bf031b949b38c7196f66d092d01"}},"752deba207af44e59488e7ad1630757e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e130da3d0b54e84b73c9fcb9dac4a6d","placeholder":"​","style":"IPY_MODEL_eb6b616ed2494e90bea7d602b42a393b","value":"tokenizer_config.json: 100%"}},"cb2a89fc42704701a11433890eac0e76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2162cb94ad224c8f820497f4b262965b","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9829da24eb4a4138933c9777c4b45112","value":48}},"1248fe97e82d413280d8dd153df0fdd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12bec8102c9e45ccaabea6e90c136aac","placeholder":"​","style":"IPY_MODEL_e1c0ce0d5d594e9c8a1806f6ce9d00dc","value":" 48.0/48.0 [00:00&lt;00:00, 1.12kB/s]"}},"47db5bf031b949b38c7196f66d092d01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e130da3d0b54e84b73c9fcb9dac4a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6b616ed2494e90bea7d602b42a393b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2162cb94ad224c8f820497f4b262965b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9829da24eb4a4138933c9777c4b45112":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12bec8102c9e45ccaabea6e90c136aac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1c0ce0d5d594e9c8a1806f6ce9d00dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6135c514f99f44c982a17957b0c46e0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b37d4afaabbf43af84b8910e22cac652","IPY_MODEL_8f0b88fdebfb48dc8cbb7267be183aa9","IPY_MODEL_6fa1b559c8a14279aab857eb9ebc0d2f"],"layout":"IPY_MODEL_1a0a9b8c31b34e85ba5328df99060e33"}},"b37d4afaabbf43af84b8910e22cac652":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7b66a5bf0534dcab44c81721b8e40fb","placeholder":"​","style":"IPY_MODEL_90fea0bc97ad468f80526cdbdf899661","value":"vocab.txt: 100%"}},"8f0b88fdebfb48dc8cbb7267be183aa9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_002c0d8b5c9444e69abd081826cf28e5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14674dd93c91451b846ceffdc3196703","value":231508}},"6fa1b559c8a14279aab857eb9ebc0d2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11a9c94862514a90a5152c873f64f15f","placeholder":"​","style":"IPY_MODEL_89009324d7ab4f47b2d917f8532caa49","value":" 232k/232k [00:00&lt;00:00, 3.74MB/s]"}},"1a0a9b8c31b34e85ba5328df99060e33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b66a5bf0534dcab44c81721b8e40fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90fea0bc97ad468f80526cdbdf899661":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"002c0d8b5c9444e69abd081826cf28e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14674dd93c91451b846ceffdc3196703":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11a9c94862514a90a5152c873f64f15f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89009324d7ab4f47b2d917f8532caa49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"effd631a21464e309989701680a16785":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d5d4e8945eb4b79bf47e52dc29e9b8e","IPY_MODEL_cf987a348d4e41b489c52af36f54589f","IPY_MODEL_d2e923d30f2543a7a65c629525de5e6f"],"layout":"IPY_MODEL_b55fb983af394a6ea24b7495a68f6ce8"}},"6d5d4e8945eb4b79bf47e52dc29e9b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34600c6962f946609e64abf12b4fca75","placeholder":"​","style":"IPY_MODEL_df89467d1edc486a9729eb56f7cf385c","value":"tokenizer.json: 100%"}},"cf987a348d4e41b489c52af36f54589f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc7a684d19a841538c0db436cc257599","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d726d3be2e1b439abaa510b4c632d6d8","value":466062}},"d2e923d30f2543a7a65c629525de5e6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a2c8ac98bf4f51bbd1644599b748e5","placeholder":"​","style":"IPY_MODEL_121234f818114e8b9652eaa5d1e587a8","value":" 466k/466k [00:00&lt;00:00, 2.11MB/s]"}},"b55fb983af394a6ea24b7495a68f6ce8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34600c6962f946609e64abf12b4fca75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df89467d1edc486a9729eb56f7cf385c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc7a684d19a841538c0db436cc257599":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d726d3be2e1b439abaa510b4c632d6d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a2c8ac98bf4f51bbd1644599b748e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"121234f818114e8b9652eaa5d1e587a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fed5f76a3d4439986e4cc70cf9271f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5cb5fe3351684dbf80a6aa706cc16bb6","IPY_MODEL_1470efa2cb00466385f2542c5b63025c","IPY_MODEL_bb8b753fab744854bdb5022daa2219cb"],"layout":"IPY_MODEL_e9871876f45243899e85e6242af07b35"}},"5cb5fe3351684dbf80a6aa706cc16bb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70206708938242fcbcf1677d85b4745a","placeholder":"​","style":"IPY_MODEL_35d07b666bbc46b3bbf10d8a6610c355","value":"config.json: 100%"}},"1470efa2cb00466385f2542c5b63025c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_970d60f6413b4194b84fd73f55da2f43","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1e5c00a0da14a7cbda1c1726aa3b7ea","value":570}},"bb8b753fab744854bdb5022daa2219cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35bbbab9b0814048bfed3c72d074c7ad","placeholder":"​","style":"IPY_MODEL_a160ed1797bd42d98f07f66db1393ec8","value":" 570/570 [00:00&lt;00:00, 22.5kB/s]"}},"e9871876f45243899e85e6242af07b35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70206708938242fcbcf1677d85b4745a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35d07b666bbc46b3bbf10d8a6610c355":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"970d60f6413b4194b84fd73f55da2f43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1e5c00a0da14a7cbda1c1726aa3b7ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35bbbab9b0814048bfed3c72d074c7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a160ed1797bd42d98f07f66db1393ec8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfd1af7ce7c0491e8052a6c3276f3f33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12f60ad939d1433a9c6a3997d9c593ca","IPY_MODEL_c88f7f44da174a5cae5f14b798472be7","IPY_MODEL_16a5c6570e6f4509a6813b1f7e0af5f0"],"layout":"IPY_MODEL_f5a802f2406542198a7ebcb97ce629c4"}},"12f60ad939d1433a9c6a3997d9c593ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63010ae35be848a7baab3d2636729629","placeholder":"​","style":"IPY_MODEL_97ca419de7b448c598748fe64afa03bf","value":"model.safetensors: 100%"}},"c88f7f44da174a5cae5f14b798472be7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a474d5dd8ac460187475e4038071d2a","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3c9cb1138ef40949439b13465e587f8","value":440449768}},"16a5c6570e6f4509a6813b1f7e0af5f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b00849d1f6b1438991bca1446e673ef0","placeholder":"​","style":"IPY_MODEL_f98e796fc6ab4d34b0206aca0b5c6a70","value":" 440M/440M [00:04&lt;00:00, 108MB/s]"}},"f5a802f2406542198a7ebcb97ce629c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63010ae35be848a7baab3d2636729629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97ca419de7b448c598748fe64afa03bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a474d5dd8ac460187475e4038071d2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3c9cb1138ef40949439b13465e587f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b00849d1f6b1438991bca1446e673ef0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f98e796fc6ab4d34b0206aca0b5c6a70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UW-u1Ru8bb3C","executionInfo":{"status":"ok","timestamp":1720795960976,"user_tz":-180,"elapsed":13169,"user":{"displayName":"try out","userId":"01859893946982656200"}},"outputId":"f5537448-e3aa-4fc8-8600-d98e72e3ffd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}],"source":["pip install tensorflow"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense\n","\n","# Define a custom layer for the Transformer encoder\n","class TransformerEncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","        # Multi-head attention layer\n","        self.mha = MultiHeadAttention(num_heads, d_model)\n","        # Feed forward network, consisting of two dense layers\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),  # First dense layer with ReLU activation\n","            Dense(d_model)  # Second dense layer without activation\n","        ])\n","\n","        # Layer normalization layers\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        # Dropout layers for regularization\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training):\n","        # Apply multi-head attention on the input\n","        attn_output = self.mha(x, x, x)\n","        # Apply dropout to the attention output if in training mode\n","        attn_output = self.dropout1(attn_output, training=training)\n","        # Add the original input to the attention output and normalize\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        # Pass the normalized output through the feed forward network\n","        ffn_output = self.ffn(out1)\n","        # Apply dropout to the feed forward network output if in training mode\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        # Add the normalized output to the feed forward network output and normalize again\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        # Return the final output of the encoder layer\n","        return out2\n"],"metadata":{"id":"epGnO2qHbo7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense"],"metadata":{"id":"ndtVz6TtbzeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(TransformerEncoderLayer, self).__init__()\n","    self.mha = MultiHeadAttention(num_heads, d_model)\n","    self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n","    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    attn_output = self.mha(x, x, x)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(x + attn_output)"],"metadata":{"id":"1GB7dGP5cIJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(TransformerDecoderLayer, self).__init__()\n","        self.mha1 = MultiHeadAttention(num_heads, d_model)\n","        self.mha2 = MultiHeadAttention(num_heads, d_model)\n","\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),\n","            Dense(d_model)\n","        ])\n","\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3, attn_weights_block1, attn_weights_block2"],"metadata":{"id":"h-nc_lkKcIMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","  attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","  attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)"],"metadata":{"id":"czcgtXRhcIO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","                 target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n","                               input_vocab_size, pe_input, rate)\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n","                               target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inp, tar, training, enc_padding_mask,\n","             look_ahead_mask, dec_padding_mask):\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","        dec_output, attention_weights = self.decoder(\n","            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)"],"metadata":{"id":"TASPTZOVcIRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","    # Initialize the training step\n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        # Training code here"],"metadata":{"id":"zyEdKv_acIUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9HZKRcecIWm","executionInfo":{"status":"ok","timestamp":1720797009238,"user_tz":-180,"elapsed":7084,"user":{"displayName":"try out","userId":"01859893946982656200"}},"outputId":"2d5f1c18-0b4e-4c83-8ecd-e6b9d0157def"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer, TFBertModel\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = TFBertModel.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423,"referenced_widgets":["1044cc152fe94becaf41fd2efd09f498","752deba207af44e59488e7ad1630757e","cb2a89fc42704701a11433890eac0e76","1248fe97e82d413280d8dd153df0fdd7","47db5bf031b949b38c7196f66d092d01","6e130da3d0b54e84b73c9fcb9dac4a6d","eb6b616ed2494e90bea7d602b42a393b","2162cb94ad224c8f820497f4b262965b","9829da24eb4a4138933c9777c4b45112","12bec8102c9e45ccaabea6e90c136aac","e1c0ce0d5d594e9c8a1806f6ce9d00dc","6135c514f99f44c982a17957b0c46e0a","b37d4afaabbf43af84b8910e22cac652","8f0b88fdebfb48dc8cbb7267be183aa9","6fa1b559c8a14279aab857eb9ebc0d2f","1a0a9b8c31b34e85ba5328df99060e33","e7b66a5bf0534dcab44c81721b8e40fb","90fea0bc97ad468f80526cdbdf899661","002c0d8b5c9444e69abd081826cf28e5","14674dd93c91451b846ceffdc3196703","11a9c94862514a90a5152c873f64f15f","89009324d7ab4f47b2d917f8532caa49","effd631a21464e309989701680a16785","6d5d4e8945eb4b79bf47e52dc29e9b8e","cf987a348d4e41b489c52af36f54589f","d2e923d30f2543a7a65c629525de5e6f","b55fb983af394a6ea24b7495a68f6ce8","34600c6962f946609e64abf12b4fca75","df89467d1edc486a9729eb56f7cf385c","cc7a684d19a841538c0db436cc257599","d726d3be2e1b439abaa510b4c632d6d8","e6a2c8ac98bf4f51bbd1644599b748e5","121234f818114e8b9652eaa5d1e587a8","9fed5f76a3d4439986e4cc70cf9271f8","5cb5fe3351684dbf80a6aa706cc16bb6","1470efa2cb00466385f2542c5b63025c","bb8b753fab744854bdb5022daa2219cb","e9871876f45243899e85e6242af07b35","70206708938242fcbcf1677d85b4745a","35d07b666bbc46b3bbf10d8a6610c355","970d60f6413b4194b84fd73f55da2f43","d1e5c00a0da14a7cbda1c1726aa3b7ea","35bbbab9b0814048bfed3c72d074c7ad","a160ed1797bd42d98f07f66db1393ec8","cfd1af7ce7c0491e8052a6c3276f3f33","12f60ad939d1433a9c6a3997d9c593ca","c88f7f44da174a5cae5f14b798472be7","16a5c6570e6f4509a6813b1f7e0af5f0","f5a802f2406542198a7ebcb97ce629c4","63010ae35be848a7baab3d2636729629","97ca419de7b448c598748fe64afa03bf","1a474d5dd8ac460187475e4038071d2a","d3c9cb1138ef40949439b13465e587f8","b00849d1f6b1438991bca1446e673ef0","f98e796fc6ab4d34b0206aca0b5c6a70"]},"id":"m0JHLVvwcIaH","executionInfo":{"status":"ok","timestamp":1720797048392,"user_tz":-180,"elapsed":26695,"user":{"displayName":"try out","userId":"01859893946982656200"}},"outputId":"fdee8bfb-ebf2-477d-aad2-444cb169bdcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1044cc152fe94becaf41fd2efd09f498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6135c514f99f44c982a17957b0c46e0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"effd631a21464e309989701680a16785"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fed5f76a3d4439986e4cc70cf9271f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd1af7ce7c0491e8052a6c3276f3f33"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["# Example sentences\n","sentences = [\"I love this product!\", \"This is a bad product.\"]\n","\n","# Tokenize sentences\n","inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"tf\")"],"metadata":{"id":"g2r6EyX3eYSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from transformers import TFBertModel, BertTokenizer\n","\n","# Load the pre-trained BERT model and tokenizer\n","model_name = 'bert-base-uncased'\n","bert_model = TFBertModel.from_pretrained(model_name)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# Example sentences for tokenization\n","sentences = [\"This is a positive example.\", \"This is a negative example.\"]\n","\n","# Tokenize the input sentences\n","inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='tf')\n","\n","# Define input layers\n","input_ids = Input(shape=(None,), dtype='int32', name=\"input_ids\")\n","attention_mask = Input(shape=(None,), dtype='int32', name=\"attention_mask\")\n","\n","# Wrap the BERT model call in a Lambda layer to handle the assertion\n","bert_output = tf.keras.layers.Lambda(lambda x: bert_model(x[0], attention_mask=x[1]).last_hidden_state[:, 0, :])([input_ids, attention_mask])\n","\n","# Add a classification layer on top\n","x = Dense(128, activation='relu')(bert_output)\n","output = Dense(1, activation='sigmoid')(x)\n","\n","# Construct the final model\n","fine_tuned_model = Model(inputs=[input_ids, attention_mask], outputs=[output])\n","\n","# Compile the model\n","fine_tuned_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Example labels for the sentences\n","labels = tf.constant([1, 0], dtype=tf.float32)  # 1 for positive, 0 for negative sentiment\n","\n","# Train the model\n","fine_tuned_model.fit([inputs['input_ids'], inputs['attention_mask']], labels, epochs=3, batch_size=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5Gu5yWqeYVV","executionInfo":{"status":"ok","timestamp":1720802078980,"user_tz":-180,"elapsed":50643,"user":{"displayName":"try out","userId":"01859893946982656200"}},"outputId":"4c4dd671-49a5-4920-ffda-a25a1e26c33a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (lambda_1), but\n","are not present in its tracked objects:\n","  <tf.Variable 'tf_bert_model_1/bert/embeddings/word_embeddings/weight:0' shape=(30522, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/embeddings/token_type_embeddings/embeddings:0' shape=(2, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/embeddings/position_embeddings/embeddings:0' shape=(512, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n","  <tf.Variable 'tf_bert_model_1/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1/1 [==============================] - 23s 23s/step - loss: 0.7143 - accuracy: 0.5000\n","Epoch 2/3\n","1/1 [==============================] - 0s 114ms/step - loss: 0.7617 - accuracy: 0.5000\n","Epoch 3/3\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6725 - accuracy: 0.5000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7df7a2275990>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Convert the BatchEncoding object to a dictionary of NumPy arrays\n","inputs_dict = {key: np.array(value) for key, value in inputs.items()}\n","\n","# Convert labels to a NumPy array\n","labels = np.array(labels)\n","\n","# Train the model\n","fine_tuned_model.fit(inputs_dict, labels, epochs=3, batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zX-0i3TtjQEu","executionInfo":{"status":"ok","timestamp":1720797992879,"user_tz":-180,"elapsed":33543,"user":{"displayName":"try out","userId":"01859893946982656200"}},"outputId":"a74e2cf2-9764-44d7-b505-a836e985aaa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n","  inputs = self._flatten_to_reference_inputs(inputs)\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 33s 33s/step - loss: 0.7881 - accuracy: 0.5000\n","Epoch 2/3\n","1/1 [==============================] - 0s 142ms/step - loss: 0.5255 - accuracy: 1.0000\n","Epoch 3/3\n","1/1 [==============================] - 0s 133ms/step - loss: 0.4427 - accuracy: 0.5000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7df7b0192710>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["test_sentences = [\"I am not sure about this product.\", \"Absolutely fantastic!\"]\n","test_inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n","\n","predictions = fine_tuned_model.predict({'input_ids': test_inputs['input_ids'], 'attention_mask': test_inputs['attention_mask']})\n","\n","# Interpret the predictions\n","for sentence, prediction in zip(test_sentences, predictions):\n","    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n","    print(f\"Sentence: '{sentence}' - Sentiment: {sentiment}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SX7e2n8VeYX6","executionInfo":{"status":"ok","timestamp":1720802183135,"user_tz":-180,"elapsed":11074,"user":{"displayName":"try out","userId":"01859893946982656200"}},"outputId":"61841a49-dcbc-48ac-e54b-982b024da6e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 10s 10s/step\n","Sentence: 'I am not sure about this product.' - Sentiment: Negative\n","Sentence: 'Absolutely fantastic!' - Sentiment: Negative\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EhWJ6bvLeYaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ReDqLRUeYeA"},"execution_count":null,"outputs":[]}]}